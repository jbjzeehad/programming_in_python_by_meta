@ Jubayer Bin Jaman 

>> Python allows for procedural, object oriented and functional programming paradigms.Working within a paradigm helps you to write code that is easy to update and create new functionality. You will likely use a combined approach that relies on procedural, object oriented and functional programming paradigms. Working with a certain approach in mind can help you write code that is easy to update and create new functionality.Most programming requires more than one approach.

D   Don't
R   Repeat
Y   Yourself

create functions that can be reused.

>> procedural programming : 
    - easy to learn,reusable,easy to understand.
    - hard to maintain,doesn't relate well,exposed data.

>> Algorithms : Algorithms are very useful in programming.An algorithm solves both small and complex problems by executing the same steps each time.An algorithm is a series of steps to solve a problem.Once created the steps of an algorithm are the same every time.

An algorithm is a set of instructions that is completed in a step-by-step way to solve a particular problem.There are many different types of algorithms that have been designed to solve all kinds of different types of problems in computer science. When writing an algorithm, it can be solved in many different ways and each can have its own pros and cons. 

Recursion > Recursion refers to a method or a function that will call itself. It is used to resolve problems by breaking the problem down into sub-problems.

Divide and conquer > This consists of two parts. The first is breaking the problem down into smaller sub-problems and the second is solving the final solution.

Dynamic programming > This is mainly used for optimization problems. It is similar to the divide and conquer algorithm in that it splits the problems into sub-problems.

Greedy algorithm > This one finds the best solution in each and every step instead of approaching optimization in a global way.

>> Pseudocode describes the algorithmic flow and does not have instructions that may be confusing to read.

>> code is measured by time and space. Big O notation.Understanding algorithmic complexity is an important aspect of optimizing code.

- Constatnt time > same time and space despite size
- Linear time > the bigger the range the biggeer the running time.A linear time algorithm means that the speed of an operation will decrease as the input increases.
- Logarithmic time  
- Quadratic time 
- Exponential time

>> Intro to Big O notation :

Big O notation is a fundamental concept in computer science and programming that helps you analyze and describe the efficiency of algorithms. It provides a standardized way of expressing how the runtime or resource usage of an algorithm grows as the size of the input data increases.
algorithms are analyzed based on their efficiency when dealing with different sizes of input data.

Big O notation is a mathematical notation that describes the upper bound or worst-case scenario for the time complexity of an algorithm. It helps us answer questions like:

How does the runtime of an algorithm change as the input data gets larger?
How does an algorithm scale with increased input size?

Big O notation is written as "O(f(n))," where "f(n)" is a function that represents the relationship between the input size (usually denoted as "n") and the algorithm's runtime or resource usage.

O(1) - Constant Time >>

In algorithms with constant time complexity, the runtime does not depend on the size of the input data. It remains constant, making it the most efficient scenario.

def access_element(arr, index):
    retutn arr[index]

No matter how large the array is, accessing an element by its index takes the same amount of time. The runtime is constant, and we denote it as O(1).

O(n) - Linear Time >>

Algorithms with linear time complexity have a runtime that grows linearly with the size of the input data. This means that if the input data doubles in size, the runtime also doubles.

def linear_search(arr,target):
    for item in arr:
        if item == target:
            return True
    return False

As the size of the list (arr) increases, the number of iterations the loop performs also increases linearly. Therefore, this algorithm has a time complexity of O(n).

O(n^2) - Quadratic Time >>

Algorithms with quadratic time complexity have runtimes that grow with the square of the input size. As the input data size increases, the runtime increases quadratically.

Example: Bubble Sort

Bubble sort has a time complexity of O(n^2). As the size of the input list (arr) increases, the number of comparisons and swaps grows quadratically.

O(log n) - Logarithmic Time >>

Algorithms with logarithmic time complexity have runtimes that grow logarithmically with the size of the input data. Logarithmic time complexity is considered very efficient.

Example: Binary search

It has a time complexity of O(log n).

>>  A Quick Breakdown

Fastest:

O(1) - Constant Time: Lightning-fast! The algorithm's speed doesn't depend on how much data you have. It's like finding your favorite book on a perfectly organized bookshelf – it takes the same amount of time, whether you have 10 books or 1,000 books.

Pretty Fast:

O(log n) - Logarithmic Time: Still quite speedy! It grows slowly as you add more data. Think of it as finding a name in a phone book by repeatedly splitting it in half – it gets faster even if the phone book gets bigger.

Moderate:

O(n) - Linear Time: Respectable speed! If you have twice as much data, it takes about twice as long. It's like looking through a list of names one by one to find a match.

Slower:

O(n log n) - Linearithmic Time: It's faster than quadratic but slower than linear. Comparable to sorting a deck of cards quickly using smart techniques.

Slower Still:

O(n^2) - Quadratic Time: Getting slower as you add data. Like checking every combination of items on a list against each other – not great for large lists.

Quite Slow:

O(2^n) - Exponential Time: Now we're talking about slow! It grows rapidly as you add data. Imagine a puzzle where you have to try every possible combination – it's really slow even for small puzzles.

Incredibly Slow:

O(n!) - Factorial Time: The slowest of all! It's like solving a complex puzzle where the number of possible arrangements explodes as you add more pieces. Practically unusable for large problems

>> Why Big O Notation Matters

Big O notation is crucial for several reasons:

-Algorithm Comparison: It allows us to objectively compare different algorithms and choose the most efficient one for a specific task.

-Performance Optimization: Understanding Big O helps identify bottlenecks in code and optimize algorithms for better performance.

-Scalability: Efficient algorithms are vital as applications and data sizes grow.

-Resource Management: In resource-constrained environments, like embedded systems, choosing efficient algorithms is essential.

-Coding Interviews: Big O notation is often tested in technical interviews and coding challenges, demonstrating your ability to analyze and optimize algorithms.

>> Analyzing Code with Big O Notation

To analyze code using Big O notation, follow these steps:

-Identify the Input Size: Determine what "n" represents in your code, often related to the size of the input data.

-Identify Loops and Iterations: Look for loops in your code, as they often determine the primary factors affecting time complexity.

-Count Operations Inside Loops: Count the number of operations inside each loop that depend on the input size "n."

-Combine Complexity: If you have nested loops, multiply their complexities to determine the overall time complexity.

-Choose the Dominant Term: In cases of combined complexity, focus on the term with the highest growth rate, as it will dominate the overall time complexity.

-Simplify: Simplify the expression as much as possible by removing constant factors.

In summary, Big O notation is a fundamental concept in computer science that helps us analyze and compare algorithms' efficiency. By understanding its basics and applying it to code, you can make informed decisions about algorithm selection and optimization, ensuring your programs run efficiently, even as data sizes grow.